# 今日のMGで話すこと
・最終的なスコア  
・前回からの修正点  

## 最終的なスコア
前回スコアが0.85だったが、特徴量の追加やパラメータチューニングを行った後、スコアは0.88まで上がった
![image](https://github.com/Yuma-Tsukakoshi/CrossViT-Summary-/assets/107422037/85faf5d6-7194-4dea-a962-4f3c060a0afd)
![image](https://github.com/Yuma-Tsukakoshi/CrossViT-Summary-/assets/107422037/455eae9f-f321-4c79-9eb2-2bf0a224b830)

## 修正点
- パラメータチューニングを各エンコーディングしたデータを用いた
チューニング後のパラメータが常識的かや過学習を起こさないかなどを以下のパラメータを調整してスコアを出してみた  

正則化の強化:  
lambda_l1の値を増やす。  
lambda_l2の値を増やす。  

ツリーの複雑さの抑制:  
num_leavesの値を減らす。  
max_depthの値を減らす。  

ランダム化の強化:
feature_fractionの値を減らす。  
bagging_fractionの値を減らす。  
bagging_freqの値を増やす。  

結果的に過学習は抑えることができたがそれに伴いスコアが低くなってしまった。  
ただ、スコアが一番高いものに関してはsubmitした後には過学習が多少は起きながらも一番スコアが高く汎用性があることが示された。  

-----------------------------------------------------------------------------------------
- EDAによる特徴量の追加
  catplotによってACTIONごとのデータの分布を見るようにした  
  0と1の分布に大きな違いが見られたり分布の境界線を見て新たに特徴量を追加した  
- frequent encodingした後に変換してからチューニングを行った。
- submit_predictionでは、X,y を分割前のデータを用いるようにした。

<大切なこと>  
コードは動かしただけ❌動かした後の評価を行う⭕️

feature_fraction は、学習に使用する特徴量の割合  
bagging_fraction は、バギングアルゴリズムで使用される特徴量の割合  
bagging_freq は、バギングアルゴリズムで使用するバッチサイズ    
min_child_samples は、子ノードを分割するために必要な最小サンプル数  
