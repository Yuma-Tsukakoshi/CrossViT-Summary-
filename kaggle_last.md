# 今日のMGで話すこと
・最終的なスコア  
・前回からの修正点  

## 最終的なスコア
前回スコアが0.85だったが、特徴量の追加やパラメータチューニングを行った後、スコアは0.88まで上がった
![image](https://github.com/Yuma-Tsukakoshi/CrossViT-Summary-/assets/107422037/85faf5d6-7194-4dea-a962-4f3c060a0afd)
![image](https://github.com/Yuma-Tsukakoshi/CrossViT-Summary-/assets/107422037/455eae9f-f321-4c79-9eb2-2bf0a224b830)

## 修正点
- パラメータチューニングを各エンコーディングしたデータを用いた

チューニングされたパラメータは常識的な値であることがわかる
→理由としては、
→過学習を抑えるために、以下のカラムを調整してスコアを出してみた
L1正則化の強度を高めるため、lambda_l1の値を増やす。
L2正則化の強度を高めるため、lambda_l2の値を増やす。


結果的に過学習は抑えることができたがそれに伴いスコアが低くなってしまった。  
ただ、submitした後には過学習が多少は起きながらも一番スコアが高く汎用性があることが示された。  

-----------------------------------------------------------------------------------------
- EDAによる特徴量の追加
  catplotによってACTIONごとのデータの分布を見るようにした  
  0と1の分布に大きな違いが見られたり分布の境界線を見て新たに特徴量を追加した  
- frequent encodingした後に変換してからチューニングを行った。
- submit_predictionでは、X,y を分割前のデータを用いるようにした。

<大切なこと>  
コードは動かしただけ❌動かした後の評価を行う⭕️

feature_fraction は、学習に使用する特徴量の割合  
bagging_fraction は、バギングアルゴリズムで使用される特徴量の割合  
bagging_freq は、バギングアルゴリズムで使用するバッチサイズ    
min_child_samples は、子ノードを分割するために必要な最小サンプル数  
